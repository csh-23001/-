# prefix Decoder和 causal Decoder和 Encoder-Decoder 区别是什么？
## 1 Causal Decoder（因果解码器）：
- 采用单向注意力掩码，确保每个输入标记只能关注过去的标记和它本身。
- 输入和输出标记通过解码器以相同的方式进行处理。
- 典型代表是GPT系列模型，如GPT-3和ChatGPT。
## 2 Prefix Decoder（前缀解码器）：
- 修正了因果编码器的掩码机制，使其能够对前缀标记执行双向注意力，并仅对生成的标记执行单向注意力。
- 类似于Encoder-Decoder，可以双向编码前缀序列并自回归地逐个预测输出标记，其中在编码和解码阶段共享相同的参数。
- 典型代表包括U-PaLM、GLM-130B等。
## 3 Encoder-Decoder（编码器-解码器）：
- 传统的Transformer模型采用的就是这种架构。
- 包含两部分：编码器和解码器，两者的参数独立。
- 编码器将输入序列处理为中间表示，而解码器基于中间表示自回归地生成目标序列。
- 典型代表是Flan-T5。
总结：
Causal Decoder使用单向注意力，Prefix Decoder在输入部分使用双向注意力，输出部分使用单向注意力，而Encoder-Decoder则分别采用双向和单向注意力。

# 大语言模型LLM的训练目标是什么？
大型语言模型（LLMs）的训练目标是学习语言的统计规律，以便能够生成或者理解人类语言。
这些模型通过大量数据的训练，像人类一样理解和生成文本，以及其他形式的内容。
它们具备从环境中推断、生成连贯且与环境相关的响应、翻译成英语以外的语言、总结文本、回答问题（一般对话和常见问题解答），甚至协助完成创造性写作或代码生成任务的能力。
总之，大语言模型旨在成为强大的自然语言处理工具，能够处理多种任务，从对话到文本生成，以满足不同应用场景的需求。

# 涌现能力是啥原因？
复杂系统中的涌现：
  首先，我们从复杂系统的角度来理解涌现。当一个复杂系统由许多微小个体相互作用构成时，这些个体在宏观层面上展现出无法由单个个体解释的特殊现象。
  例如，雪花的形成、堵车、动物迁徙等都是涌现现象。这些看似简单的个体相互作用，聚合在一起，产生了意想不到的结果，类似于水循环系统
大型语言模型中的涌现：
  模型规模与涌现能力之间存在关联。不同任务对模型大小的要求不同，但一般来说，模型达到一定规模后，涌现能力会显现。
  虽然涌现的具体原因尚不完全清楚，但模型规模、训练数据量和任务类型等因素都在其中起到作用。
